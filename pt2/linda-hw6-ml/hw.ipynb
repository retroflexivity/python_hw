{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sys import getsizeof\n",
    "from time import time\n",
    "\n",
    "def mb_size_str(obj: object, name: str):\n",
    "    return f\"{name} size is {round(getsizeof(obj) / pow(1024, 2), 2)}MB\"\n",
    "\n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "train_file = \"train.csv\"\n",
    "train_df = pd.read_csv(train_file)\n",
    "INPUT_DATASET_SIZE = train_df.shape[0]\n",
    "b_time = time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'TF_IDF': True,\n",
    "    'LEMMATIZE': True,\n",
    "    'DO_STEMMING': True,\n",
    "    'STOPLIST': True,\n",
    "    'LOWFREQ_FILTER': True,\n",
    "    'LOWFREQ_TRESHOLD': 5,\n",
    "    'TEST_RATIO': 0.5, # ratio of train samples that will go as test ones\n",
    "    'MODEL': \"6 Gensim Continuous Skipgram\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/retroflexivity/.nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/retroflexivity/.nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 25000/25000 [01:04<00:00, 385.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing low freq tokens (freq <= 5)...\n",
      "Total forms: 128069\n",
      "Total forms to be removed 105756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 25000/25000 [00:02<00:00, 8378.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total token amount:\n",
      "old 3071806\n",
      "new 2920990\n",
      "4.91% total less\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # \n",
    "#           Preprocessing           #\n",
    "# # # # # # # # # # # # # # # # # # # \n",
    "\n",
    "import src.preprocessing as prep\n",
    "\n",
    "freq_dict = {}\n",
    "token_amount_counter = [0]\n",
    "\n",
    "print(\"Preprocessing...\")\n",
    "for row_id in tqdm(range(INPUT_DATASET_SIZE)):\n",
    "    text = train_df.loc[row_id, \"text\"]\n",
    "    train_df.loc[row_id, \"text\"] = prep.preprocess(\n",
    "        text, cfg, freq_dict, token_amount_counter\n",
    "    )\n",
    "\n",
    "if cfg['LOWFREQ_FILTER']:\n",
    "    forms_to_remove = set(w for (w, freq) in freq_dict.items() if freq <= cfg['LOWFREQ_TRESHOLD'])\n",
    "    new_token_amount_counter = [0]\n",
    "    print(f\"Removing low freq tokens (freq <= {cfg['LOWFREQ_TRESHOLD']})...\")\n",
    "    print(f\"Total forms: {len(freq_dict.items())}\")\n",
    "    print(f\"Total forms to be removed {len(forms_to_remove)}\")\n",
    "    for row_id in tqdm(range(INPUT_DATASET_SIZE)):\n",
    "        text = train_df.loc[row_id, \"text\"]\n",
    "        train_df.loc[row_id, \"text\"] = prep.remove_lowfreq(text, forms_to_remove, new_token_amount_counter)\n",
    "    shrink_percent = round(100 * (token_amount_counter[0] - new_token_amount_counter[0]) / token_amount_counter[0], 2)\n",
    "    print(f\"Total token amount:\\nold {token_amount_counter[0]}\\nnew {new_token_amount_counter[0]}\\n{shrink_percent}% total less\")\n",
    "\n",
    "train_df.to_csv(f\"train_cleaned.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : TF-IDF matrix...\n",
      "INFO : Матрица на 25000 документов и 22102 термов\n"
     ]
    }
   ],
   "source": [
    "from src.tfidf import get_matrix\n",
    "tfidf, matrix = get_matrix(train_df['text'].to_list()) if cfg['TF_IDF'] else (None, None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading projection weights from 6 Gensim Continuous Skipgram/model.bin\n",
      "INFO : KeyedVectors lifecycle event {'msg': 'loaded (302866, 300) matrix of type float32 from 6 Gensim Continuous Skipgram/model.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-02-13T23:50:23.012651', 'gensim': '4.2.0', 'python': '3.9.2 (default, Feb 28 2021, 17:03:44) \\n[GCC 10.2.1 20210110]', 'platform': 'Linux-5.10.159-20945-g4390861bfc33-x86_64-with-glibc2.31', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE MODEL\n",
    "from nltk.tokenize import word_tokenize\n",
    "w2v_file = f\"{cfg['MODEL']}/model.bin\"\n",
    "model: gensim.models.keyedvectors.KeyedVectors = gensim.models.KeyedVectors.load_word2vec_format(w2v_file, binary=True)\n",
    "VECTOR_SIZE = model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n",
      "Calculating mean review vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 25000/25000 [00:59<00:00, 419.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.vectorizers import get_tfidf_vector\n",
    "\n",
    "def text_vector(text: str, text_id: int) -> np.ndarray:\n",
    "    \"\"\"Compute the normalized weighted mean w2v vector for a given text\"\"\"\n",
    "\n",
    "    tokens = [token for token in word_tokenize(text) if token in model]\n",
    "    # Compute the word2vec vectors for each word in the text\n",
    "    vectors = [model.get_vector(tkn) for tkn in tokens]\n",
    "    if cfg['TF_IDF']:\n",
    "        # Compute the tf-idf values for each word in the text.\n",
    "        tfidf_vals = get_tfidf_vector(tokens, text_id, matrix, tfidf)\n",
    "        # Compute the weighted vectors by multiplying the word2vec vectors by the tf-idf values\n",
    "        weighted_vecs = np.array([vec * factor for vec, factor in zip(vectors, tfidf_vals)])\n",
    "    else:\n",
    "        weighted_vecs = np.array(vectors)\n",
    "\n",
    "    text_sum_vec = np.sum(weighted_vecs, axis=0)\n",
    "    norm_vec = text_sum_vec / np.linalg.norm(text_sum_vec)\n",
    "    \n",
    "    return norm_vec\n",
    "\n",
    "review_vectors = np.empty((INPUT_DATASET_SIZE, VECTOR_SIZE), dtype=float)\n",
    "print(review_vectors.shape)\n",
    "print(\"Calculating mean review vectors...\")\n",
    "for row_id in tqdm(range(INPUT_DATASET_SIZE)):\n",
    "    review_vectors[row_id] = text_vector(train_df.loc[row_id, \"text\"], row_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.2205810546875 MB\n"
     ]
    }
   ],
   "source": [
    "# trying not to run out of memory on my potatoe (╥﹏╥)\n",
    "print(getsizeof(review_vectors) / pow(1024, 2), \"MB\")\n",
    "#model = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Total answers: 25000\n",
      "    Total reviews: 25000\n",
      "    Review vector size: 300\n",
      "    Answers splitted: 12500 / 12500\n",
      "    Input splitted: 12500 / 12500\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "answers = train_df['answer'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    review_vectors,\n",
    "    answers,\n",
    "    test_size=cfg['TEST_RATIO']\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "    Total answers: {answers.shape[0]}\n",
    "    Total reviews: {review_vectors.shape[0]}\n",
    "    Review vector size: {review_vectors.shape[1]}\n",
    "    Answers splitted: {y_train.shape[0]} / {y_test.shape[0]}\n",
    "    Input splitted: {X_train.shape[0]} / {X_test.shape[0]}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Score on\n",
      "      - train  data : 0.79344\n",
      "      - test   data : 0.7924\n",
      "    Test/train size : 0.5\n",
      "    Total time      : 10m 56s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from pprint import pprint\n",
    "\n",
    "reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "diff = time() - b_time\n",
    "\n",
    "results = {}\n",
    "results.update(cfg)\n",
    "print(f\"\"\"\n",
    "    Score on\n",
    "      - train  data : {reg.score(X_train, y_train)}\n",
    "      - test   data : {reg.score(X_test, y_test)}\n",
    "    Test/train size : {cfg['TEST_RATIO']}\n",
    "    Total time      : {\"{:.0f}m {:.0f}s\".format(*divmod(diff, 60))}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "      - train  data : 0.83735\n",
    "      - test   data : 0.8364\n",
    "    Test/train size : 0.2\n",
    "\n",
    "    # + tfidf\n",
    "      - train  data : 0.8083555555555556\n",
    "      - test   data : 0.7952\n",
    "    Test/train size : 0.1\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 25000/25000 [02:04<00:00, 201.36it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_vector(text: str) -> np.ndarray:\n",
    "    tokens = [token for token in word_tokenize(text) if token in model]\n",
    "    # Compute the word2vec vectors for each word in the text\n",
    "    vectors = np.array([model.get_vector(tkn) for tkn in tokens])\n",
    "    sum_vector = np.sum(vectors, axis=0)\n",
    "    norm_vector = sum_vector / np.linalg.norm(sum_vector)\n",
    "    return norm_vector\n",
    "\n",
    "test_file = \"test.csv\"\n",
    "test_df = pd.read_csv(test_file)\n",
    "TEST_DATASET_SIZE = test_df.shape[0]\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # \n",
    "#             Processing            #\n",
    "# # # # # # # # # # # # # # # # # # # \n",
    "print(\"Processing...\")\n",
    "result = test_df[[\"id\"]].copy()\n",
    "for row_id in tqdm(range(TEST_DATASET_SIZE)):\n",
    "    text = test_df.loc[row_id, \"text\"]\n",
    "    test_df.loc[row_id, \"text\"] = prep.preprocess(\n",
    "        text, cfg, freq_dict, token_amount_counter\n",
    "    )\n",
    "    result.loc[row_id, \"answer\"] = int(reg.predict([get_vector(text)])[0])\n",
    "test_df.to_csv(f\"test_cleaned.csv\", index=False)\n",
    "result[\"answer\"] = result[\"answer\"].astype(int)\n",
    "result.to_csv(f\"result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
